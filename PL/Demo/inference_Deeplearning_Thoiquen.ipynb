{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0BZ-2Ck9iI6",
        "outputId": "a93cf59e-e6e0-4a9f-cceb-b5bc149de796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install cmath\n",
        "!pip install torch\n",
        "!pip install sklearn\n",
        "!pip install pandas\n",
        "!pip install librosa\n",
        "!pip install numpy\n",
        "!pip install speechpy\n",
        "!pip install transformers\n",
        "!pip install jiwer\n",
        "!pip install pyctcdecode\n",
        "\n",
        "from pyctcdecode import build_ctcdecoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8w-q5Bm-Et1",
        "outputId": "13c5d08d-d641-4ba8-dce5-5032bee79bd3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=8cd64f0b82a21a72b3d42c01552ee303e050ac5175075b1719e827a61c3c7675\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/25/f7/1cc0956978ae479e75140219088deb7a36f60459df242b1a72\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.8/dist-packages (0.8.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (21.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.7.3)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (3.0.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from librosa) (0.4.2)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.0.2)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.8/dist-packages (from librosa) (0.11.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.2.0)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (0.56.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->librosa) (6.0.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->librosa) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->librosa) (3.0.9)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.0->librosa) (2.25.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.8/dist-packages (from soundfile>=0.10.2->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.43.0->librosa) (3.11.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting speechpy\n",
            "  Downloading speechpy-2.4-py2.py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from speechpy) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from speechpy) (1.21.6)\n",
            "Installing collected packages: speechpy\n",
            "Successfully installed speechpy-2.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-2.5.1-py3-none-any.whl (15 kB)\n",
            "Collecting levenshtein==0.20.2\n",
            "  Downloading Levenshtein-0.20.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
            "  Downloading rapidfuzz-2.13.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, levenshtein, jiwer\n",
            "Successfully installed jiwer-2.5.1 levenshtein-0.20.2 rapidfuzz-2.13.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyctcdecode\n",
            "  Downloading pyctcdecode-0.5.0-py2.py3-none-any.whl (39 kB)\n",
            "Collecting pygtrie<3.0,>=2.1\n",
            "  Downloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
            "Collecting hypothesis<7,>=6.14\n",
            "  Downloading hypothesis-6.65.0-py3-none-any.whl (402 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.2/402.2 KB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from pyctcdecode) (1.21.6)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/dist-packages (from hypothesis<7,>=6.14->pyctcdecode) (22.2.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from hypothesis<7,>=6.14->pyctcdecode) (2.4.0)\n",
            "Collecting exceptiongroup>=1.0.0\n",
            "  Downloading exceptiongroup-1.1.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pygtrie, exceptiongroup, hypothesis, pyctcdecode\n",
            "Successfully installed exceptiongroup-1.1.0 hypothesis-6.65.0 pyctcdecode-0.5.0 pygtrie-2.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyctcdecode.language_model:kenlm python bindings are not installed. Most likely you want to install it using: pip install https://github.com/kpu/kenlm/archive/master.zip\n",
            "WARNING:pyctcdecode.decoder:kenlm python bindings are not installed. Most likely you want to install it using: pip install https://github.com/kpu/kenlm/archive/master.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube\n",
        "!pip install pydub\n",
        "!pip install moviepy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlRE7ILDL8OJ",
        "outputId": "01bdaf08-b58a-494e-ecce-3a71d3d59ff3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytube\n",
            "  Downloading pytube-12.1.2-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-12.1.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.8/dist-packages (0.2.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from moviepy) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.8/dist-packages (from moviepy) (4.64.1)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.8/dist-packages (from moviepy) (2.9.0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.8/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from imageio<3.0,>=2.1.2->moviepy) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imageio-ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUFDbVyTMI2L",
        "outputId": "6295e6d4-8750-4775-fe0c-0c24ddae28e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting imageio-ffmpeg\n",
            "  Downloading imageio_ffmpeg-0.4.8-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imageio-ffmpeg\n",
            "Successfully installed imageio-ffmpeg-0.4.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/PL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35HaP_rI-CJ8",
        "outputId": "bf298f1c-e836-4a90-98bf-f7fa454cc6ce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/PL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from model import Net\n",
        "# from cProfile import label\n",
        "# from infer import phonetic_embedding\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from phonetic_encoder import Phonetic_encoder\n",
        "from acoustic_encoder import Acoustic_encoder\n",
        "from linguistic_encoder import Linguistic_encoder\n",
        "from char_embedding import text_to_tensor, clean_corpus\n",
        "from help import wav_norm, Atention\n",
        "import numpy as np\n",
        "# from help import beam_search_decoding\n",
        "from model import Acoustic_Phonetic_Linguistic\n",
        "# from dataloader import MDD_Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from jiwer import wer, cer\n",
        "tokenizer = Wav2Vec2Processor.from_pretrained(\"pretrained_finetuned\")\n",
        "\n",
        "net = Acoustic_Phonetic_Linguistic()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hu_pd_h7-JfP",
        "outputId": "3fe999d6-f3af-4b1d-cc73-3d023e7a833c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/configuration_utils.py:369: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPznFy-VPVMc",
        "outputId": "ebb81e95-3966-4bf4-bde7-e344d004dd0d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import re\n",
        "import numpy\n",
        "from pytube import YouTube, Playlist\n",
        "import pytube\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.functional as F\n",
        "import subprocess\n",
        "\n",
        "\n",
        "# from moviepy.editor import *\n",
        "import glob\n",
        "from pydub import AudioSegment\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "#download setting\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# video index to continue crawling, index = 0 means the first video\n",
        "START_INDEX = 0\n",
        "\n",
        "# importing the module\n",
        "from pytube import YouTube\n",
        "\n",
        "from pytube import YouTube\n",
        "yt = YouTube('https://www.youtube.com/watch?v=yi3Gl3Djh3s&ab_channel=M%E1%BA%A8NNGUY%E1%BB%84N')\n",
        "yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution')[-1].download()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KAQIzFq6K4LD",
        "outputId": "f0a59908-4960-4e36-f0d3-153d9fd360d0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/LYRICS VIDEO  THÓI QUEN  HOÀNG DŨNG.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = glob.glob(\"*.mp4\")\n",
        "for i in range(len(x)):\n",
        "  os.rename(x[0], 'test.mp4')"
      ],
      "metadata": {
        "id": "CYWVuicSk8yI"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "command = \"ffmpeg -i test.mp4 -ab 160k -ac 2 -ar 44100 -vn /content/test.wav\"\n",
        "subprocess.call(command, shell=True)\n",
        "\n",
        "i = 'test.wav'\n",
        "y, sr = torchaudio.load(i)       \n",
        "y_16k = F.resample(y, sr, 16000)\n",
        "y_16k = y_16k.numpy()\n",
        "y_mono = librosa.to_mono(y_16k)\n",
        "sf.write(i, y_mono, 16000)"
      ],
      "metadata": {
        "id": "lt4O7n3zPvzc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "t1 = 136 * 1000 #Works in milliseconds\n",
        "t2 = 150 * 1000\n",
        "newAudio = AudioSegment.from_wav(\"test.wav\")\n",
        "newAudio = newAudio[t1:t2]\n",
        "newAudio.export('test.wav', format=\"wav\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dopqb4I7P-Z8",
        "outputId": "ce98c94c-57c0-4fcb-f9f0-51319c930cd6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_io.BufferedRandom name='test.wav'>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/PL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca6PyFHvQqM-",
        "outputId": "e166ce9d-45fd-4156-e4b2-ab17143ef06e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/PL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from infer import phonetic_embedding\n",
        "\n",
        "net = torch.load('/content/drive/MyDrive/PL/MDD_Checkpoint/song_checkpoint.pth')\n",
        "net.eval().to('cuda')\n",
        "path = '/content/test.wav'\n",
        "lyrics = 'Em vẫn thường buộc tóc thật xinh đợi trông những lần đưa đón Vẫn cười tươi vì những câu đùa cỏn con Thức thật khuya chờ dòng tin nhắn chúc em ngủ ngon'\n",
        "phonetic = phonetic_embedding(path).squeeze(0)\n",
        "linguistic = text_to_tensor(lyrics)\n",
        "linguistic = torch.tensor(linguistic)\n",
        "phonetic = phonetic.to('cuda')\n",
        "linguistic = linguistic.to('cuda')\n",
        "phonetic = phonetic.unsqueeze(0)\n",
        "linguistic = linguistic.unsqueeze(0)\n",
        "outputs = net(phonetic, linguistic)\n",
        "outputs = outputs.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "YM34oXF2-YWc"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"/content/output.npy\", outputs)"
      ],
      "metadata": {
        "id": "8vc49LTQ-rEm"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# /content/drive/MyDrive/output_alignment_fusion/3130303538355f3338.npy\n",
        "# !pip install torch\n",
        "# from torch.utils.data import Dataset\n",
        "# import matplotlib.pyplot as plt\n",
        "# import os\n",
        "# import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "# import numpy as np\n",
        "# from char_embedding import tensor_to_text\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "\n",
        "\n",
        "x = np.load(\"/content/output.npy\")\n",
        "\n",
        "# print(x)\n",
        "\n",
        "x = torch.tensor(x)\n",
        "predicted_ids = torch.argmax(torch.tensor(x), dim=1)\n",
        "\n",
        "\n",
        "x = torch.log_softmax(x, dim=-1)\n",
        "x = x.cpu().detach()\n",
        "\n",
        "\n",
        "labels = ('ắ', 'ồ', 'z', 'ứ', 'ỡ', 'ì', 'x', 'ặ', 'u', 'ẹ', 'd', 'ỵ', 'r', 'p', 't', 'ỳ', 'ẩ', 'f', 'ó', 'á', 'v', 'ã', 'i', 'ư', 'ở', 'ễ', 'ụ', 'ú', 'ũ', ' ', 'ă', 'é', 'ằ', 'a', 'ấ', 'ờ', 'ữ', 'ớ', 'n', 'ý', 's', 'h', 'ơ', 'ị', 'l', 'c', 'k', 'ỷ', 'ỗ', 'ế', 'ẻ', 'ợ', 'ẫ', 'í', 'ỏ', 'ủ', 'g', 'q', 'j', 'ò', 'ỹ', 'ự', 'ô', 'b', 'y', 'ĩ', 'ỉ', 'ẵ', 'ầ', 'ê', 'ộ', 'ậ', 'm', 'ń', 'o', 'ọ', 'đ', 'ẽ', 'ử', 'à', 'è', 'e', 'ẳ', 'ổ', 'ù', 'w', 'ả', 'ạ', 'â', 'ệ', 'ề', 'õ', 'ố', 'ể', 'ừ',)\n",
        "\n",
        "def clean_corpus(str1):\n",
        "    res1 = \"\"\n",
        "    for i in str1:\n",
        "        if i.isalpha() or i==\" \":\n",
        "            res1 = \"\".join([res1, i])\n",
        "    return res1.lower()\n",
        "transcript = \" \" + lyrics + \"\"\n",
        "transcript = clean_corpus(transcript)\n",
        "dictionary = {c: i for i, c in enumerate(labels)}\n",
        "\n",
        "tokens = [dictionary[c] for c in transcript]\n",
        "\n",
        "\n",
        "def get_trellis(x, tokens, blank_id=95):\n",
        "    num_frame = x.size(0)\n",
        "    num_tokens = len(tokens)\n",
        "\n",
        "    # Trellis has extra diemsions for both time axis and tokens.\n",
        "    # The extra dim for tokens represents <SoS> (start-of-sentence)\n",
        "    # The extra dim for time axis is for simplification of the code.\n",
        "    trellis = torch.empty((num_frame + 1, num_tokens + 1))\n",
        "    trellis[0, 0] = 0\n",
        "    trellis[1:, 0] = torch.cumsum(x[:, 0], 0)\n",
        "    trellis[0, -num_tokens:] = -float(\"inf\")\n",
        "    trellis[-num_tokens:, 0] = float(\"inf\")\n",
        "\n",
        "    for t in range(num_frame):\n",
        "        trellis[t + 1, 1:] = torch.maximum(\n",
        "            # Score for staying at the same token\n",
        "            trellis[t, 1:] + x[t, blank_id],\n",
        "            # Score for changing to the next token\n",
        "            trellis[t, :-1] + x[t, tokens],\n",
        "        )\n",
        "    return trellis\n",
        "\n",
        "\n",
        "trellis = get_trellis(x, tokens)\n",
        "\n",
        "from dataclasses import dataclass\n",
        "@dataclass\n",
        "class Point:\n",
        "  token_index: int\n",
        "  time_index: int\n",
        "  score: float\n",
        "\n",
        "\n",
        "def backtrack(trellis, emission, tokens, blank_id=95):\n",
        "  # Note:\n",
        "  # j and t are indices for trellis, which has extra dimensions \n",
        "  # for time and tokens at the beginning.\n",
        "  # When refering to time frame index `T` in trellis,\n",
        "  # the corresponding index in emission is `T-1`.\n",
        "  # Similarly, when refering to token index `J` in trellis,\n",
        "  # the corresponding index in transcript is `J-1`.\n",
        "  j = trellis.size(1) - 1\n",
        "  t_start = torch.argmax(trellis[:, j]).item()\n",
        "\n",
        "  path = []\n",
        "  for t in range(t_start, 0, -1):\n",
        "    # 1. Figure out if the current position was stay or change\n",
        "    # Note (again):\n",
        "    # `emission[J-1]` is the emission at time frame `J` of trellis dimension.\n",
        "    # Score for token staying the same from time frame J-1 to T.\n",
        "    stayed = trellis[t-1, j] + emission[t-1, blank_id]\n",
        "    # Score for token changing from C-1 at T-1 to J at T.\n",
        "    changed = trellis[t-1, j-1] + emission[t-1, tokens[j-1]]\n",
        "\n",
        "    # 2. Store the path with frame-wise probability.\n",
        "    prob = emission[t-1, tokens[j-1] if changed > stayed else 0].exp().item()\n",
        "    # Return token index and time index in non-trellis coordinate.\n",
        "    path.append(Point(j-1, t-1, prob))\n",
        "\n",
        "    # 3. Update the token\n",
        "    if changed > stayed:\n",
        "      j -= 1\n",
        "      if j == 0:\n",
        "        break\n",
        "  else:\n",
        "    raise ValueError('Failed to align')\n",
        "  return path[::-1]\n",
        "\n",
        "path = backtrack(trellis, x, tokens)\n",
        "@dataclass\n",
        "class Segment:\n",
        "    label: str\n",
        "    start: int\n",
        "    end: int\n",
        "    score: float\n",
        "    \n",
        "\n",
        "    def __repr__(self):\n",
        "      return f\"{self.start} --> {self.end} {self.label} \\n\"\n",
        "\n",
        "    @property\n",
        "    def length(self):\n",
        "        return self.end - self.start\n",
        "\n",
        "\n",
        "def merge_repeats(path):\n",
        "    i1, i2 = 0, 0\n",
        "    segments = []\n",
        "    while i1 < len(path):\n",
        "        while i2 < len(path) and path[i1].token_index == path[i2].token_index:\n",
        "            i2 += 1\n",
        "        score = sum(path[k].score for k in range(i1, i2)) / (i2 - i1)\n",
        "        segments.append(\n",
        "            Segment(\n",
        "                transcript[path[i1].token_index],\n",
        "                path[i1].time_index,\n",
        "                path[i2 - 1].time_index + 1,\n",
        "                score,\n",
        "            )\n",
        "        )\n",
        "        i1 = i2\n",
        "    return segments\n",
        "\n",
        "\n",
        "segments = merge_repeats(path)\n",
        "\n",
        "# Merge words\n",
        "def merge_words(segments, separator=\" \"):\n",
        "    words = []\n",
        "    i1, i2 = 0, 0\n",
        "    while i1 < len(segments):\n",
        "        if i2 >= len(segments) or segments[i2].label == separator:\n",
        "            if i1 != i2:\n",
        "                segs = segments[i1:i2]\n",
        "                word = \"\".join([seg.label for seg in segs])\n",
        "                score = sum(seg.score * seg.length for seg in segs) / sum(seg.length for seg in segs)\n",
        "                words.append(Segment(word, segments[i1].start*20, segments[i2 - 1].end*20, score))\n",
        "            i1 = i2 + 1\n",
        "            i2 = i1\n",
        "        else:\n",
        "            i2 += 1\n",
        "    return words\n",
        "\n",
        "\n",
        "word_segments = merge_words(segments)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcI7rHUh_Bbz",
        "outputId": "2a0f0758-e349-4034-8de5-c6d9d411f9cc"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-40-d1039fe054e0>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  predicted_ids = torch.argmax(torch.tensor(x), dim=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_millis(milliseconds):\n",
        "  millis = milliseconds\n",
        "  millisecond = int(millis%1000)\n",
        "  millisecond = str(millisecond)\n",
        "  if len(millisecond)==0:\n",
        "    millisecond = '000'\n",
        "  elif len(millisecond) == 1:\n",
        "    millisecond = '00' + millisecond\n",
        "  elif len(millisecond) == 2:\n",
        "    millisecond = '0' + millisecond\n",
        "\n",
        "  seconds=(millis/1000)%60\n",
        "  seconds = int(seconds)\n",
        "  seconds = str(seconds)\n",
        "  if len(seconds) == 1:\n",
        "    seconds = '0' + seconds\n",
        "  minutes=(millis/(1000*60))%60\n",
        "  \n",
        "  minutes = int(minutes)\n",
        "  minutes = str(minutes)\n",
        "  if len(minutes) == 1:\n",
        "    minutes = '0' + minutes\n",
        "\n",
        "  hours=(millis/(1000*60*60))%24\n",
        "  hours = int(hours)\n",
        "  hours = str(hours)\n",
        "  if len(hours) == 1:\n",
        "    hours = '0' + hours\n",
        "\n",
        "  return (hours + \":\" + minutes + \":\" +  seconds + \",\" + millisecond)"
      ],
      "metadata": {
        "id": "u5EQ36cu_Dat"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"/content/thoiquen.srt\", \"a\")\n",
        "for i in range(len(word_segments)):\n",
        " \n",
        "  if (i==0):\n",
        "    start = str(str(word_segments[i])).split(\" \")[0]\n",
        "    end_before = int(str(str(word_segments[i])).split(\" \")[2])\n",
        "    word = str(str(word_segments[i])).split(\" \")[3]\n",
        "    if int(start)>=200:\n",
        "      start = int(start)-200\n",
        "    else:\n",
        "      start = 0\n",
        "    start_next = int(str(word_segments[i+1]).split(\" \")[0])\n",
        "    end = (2*end_before + start_next)/3\n",
        "  elif i==len(word_segments)-1:\n",
        "    end_before = int(str(word_segments[i-1]).split(\" \")[2])\n",
        "    start_next = int(str(word_segments[i]).split(\" \")[0])\n",
        "    end = int(str(word_segments[i]).split(\" \")[2]) + 250\n",
        "    start = (2*end_before+start_next)/3\n",
        "    word = str(word_segments[i]).split(\" \")[3]\n",
        "  else:\n",
        "    end_before = int(str(word_segments[i-1]).split(\" \")[2])\n",
        "    start_next = int(str(word_segments[i+1]).split(\" \")[0])\n",
        "    end = int(str(word_segments[i]).split(\" \")[2])\n",
        "    start = int(str(word_segments[i]).split(\" \")[0])\n",
        "    start = (2*end_before+start)/3\n",
        "    end = (2*end + start_next)/3\n",
        "    word = str(word_segments[i]).split(\" \")[3]\n",
        "  \n",
        "  start = (format_millis(start))\n",
        "  end = (format_millis(end))\n",
        "  one_line = str(i+1) + \"\\n\" + start + \" --> \" + end + \"\\n\" + str(word) + \"\\n\\n\"\n",
        "  print(one_line)\n",
        "  f.write(one_line)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaCYVgqQ_Ggq",
        "outputId": "5b520b29-421b-40f2-9d48-8205911462fb"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "00:00:00,580 --> 00:00:00,946\n",
            "em\n",
            "\n",
            "\n",
            "2\n",
            "00:00:00,946 --> 00:00:01,093\n",
            "vẫn\n",
            "\n",
            "\n",
            "3\n",
            "00:00:01,093 --> 00:00:01,433\n",
            "thường\n",
            "\n",
            "\n",
            "4\n",
            "00:00:01,433 --> 00:00:01,913\n",
            "buộc\n",
            "\n",
            "\n",
            "5\n",
            "00:00:01,913 --> 00:00:02,240\n",
            "tóc\n",
            "\n",
            "\n",
            "6\n",
            "00:00:02,240 --> 00:00:02,806\n",
            "thật\n",
            "\n",
            "\n",
            "7\n",
            "00:00:02,806 --> 00:00:03,113\n",
            "xinh\n",
            "\n",
            "\n",
            "8\n",
            "00:00:03,113 --> 00:00:03,666\n",
            "đợi\n",
            "\n",
            "\n",
            "9\n",
            "00:00:03,666 --> 00:00:03,960\n",
            "trông\n",
            "\n",
            "\n",
            "10\n",
            "00:00:03,960 --> 00:00:04,346\n",
            "những\n",
            "\n",
            "\n",
            "11\n",
            "00:00:04,346 --> 00:00:04,560\n",
            "lần\n",
            "\n",
            "\n",
            "12\n",
            "00:00:04,560 --> 00:00:04,793\n",
            "đưa\n",
            "\n",
            "\n",
            "13\n",
            "00:00:04,793 --> 00:00:05,300\n",
            "đón\n",
            "\n",
            "\n",
            "14\n",
            "00:00:05,300 --> 00:00:05,746\n",
            "vẫn\n",
            "\n",
            "\n",
            "15\n",
            "00:00:05,746 --> 00:00:06,200\n",
            "cười\n",
            "\n",
            "\n",
            "16\n",
            "00:00:06,200 --> 00:00:06,706\n",
            "tươi\n",
            "\n",
            "\n",
            "17\n",
            "00:00:06,706 --> 00:00:07,066\n",
            "vì\n",
            "\n",
            "\n",
            "18\n",
            "00:00:07,066 --> 00:00:07,400\n",
            "những\n",
            "\n",
            "\n",
            "19\n",
            "00:00:07,400 --> 00:00:07,806\n",
            "câu\n",
            "\n",
            "\n",
            "20\n",
            "00:00:07,806 --> 00:00:07,986\n",
            "đùa\n",
            "\n",
            "\n",
            "21\n",
            "00:00:07,986 --> 00:00:08,206\n",
            "cỏn\n",
            "\n",
            "\n",
            "22\n",
            "00:00:08,206 --> 00:00:08,740\n",
            "con\n",
            "\n",
            "\n",
            "23\n",
            "00:00:08,740 --> 00:00:09,206\n",
            "thức\n",
            "\n",
            "\n",
            "24\n",
            "00:00:09,206 --> 00:00:09,613\n",
            "thật\n",
            "\n",
            "\n",
            "25\n",
            "00:00:09,613 --> 00:00:10,066\n",
            "khuya\n",
            "\n",
            "\n",
            "26\n",
            "00:00:10,066 --> 00:00:10,286\n",
            "chờ\n",
            "\n",
            "\n",
            "27\n",
            "00:00:10,286 --> 00:00:10,520\n",
            "dòng\n",
            "\n",
            "\n",
            "28\n",
            "00:00:10,520 --> 00:00:10,766\n",
            "tin\n",
            "\n",
            "\n",
            "29\n",
            "00:00:10,766 --> 00:00:11,086\n",
            "nhắn\n",
            "\n",
            "\n",
            "30\n",
            "00:00:11,086 --> 00:00:11,366\n",
            "chúc\n",
            "\n",
            "\n",
            "31\n",
            "00:00:11,366 --> 00:00:11,620\n",
            "em\n",
            "\n",
            "\n",
            "32\n",
            "00:00:11,620 --> 00:00:12,066\n",
            "ngủ\n",
            "\n",
            "\n",
            "33\n",
            "00:00:12,066 --> 00:00:12,510\n",
            "ngon\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -i /content/lyric_align.jpg -i \"/content/test.wav\" -sub_charenc utf-8 -i \"/content/thoiquen.srt\" \"/content/OUTPUT.mp4\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPQuNbKWD9Om",
        "outputId": "31c348bc-c6df-467a-db3a-8a7ab10d93d8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
            "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 31.100 / 56. 31.100\n",
            "  libavcodec     58. 54.100 / 58. 54.100\n",
            "  libavformat    58. 29.100 / 58. 29.100\n",
            "  libavdevice    58.  8.100 / 58.  8.100\n",
            "  libavfilter     7. 57.100 /  7. 57.100\n",
            "  libavresample   4.  0.  0 /  4.  0.  0\n",
            "  libswscale      5.  5.100 /  5.  5.100\n",
            "  libswresample   3.  5.100 /  3.  5.100\n",
            "  libpostproc    55.  5.100 / 55.  5.100\n",
            "Input #0, image2, from '/content/lyric_align.jpg':\n",
            "  Duration: 00:00:00.04, start: 0.000000, bitrate: 5082 kb/s\n",
            "    Stream #0:0: Video: mjpeg (Progressive), yuvj444p(pc, bt470bg/unknown/unknown), 612x357 [SAR 300:300 DAR 12:7], 25 tbr, 25 tbn, 25 tbc\n",
            "\u001b[0;33mGuessed Channel Layout for Input Stream #1.0 : mono\n",
            "\u001b[0mInput #1, wav, from '/content/test.wav':\n",
            "  Duration: 00:00:14.00, bitrate: 256 kb/s\n",
            "    Stream #1:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s\n",
            "Input #2, srt, from '/content/thoiquen.srt':\n",
            "  Duration: N/A, bitrate: N/A\n",
            "    Stream #2:0: Subtitle: subrip\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
            "  Stream #1:0 -> #0:1 (pcm_s16le (native) -> aac (native))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x5577dcad8000] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x5577dcad8000] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x5577dcad8000] \u001b[0mprofile High 4:4:4 Predictive, level 3.0, 4:4:4 8-bit\n",
            "\u001b[1;36m[libx264 @ 0x5577dcad8000] \u001b[0m264 - core 155 r2917 0a84d98 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x1:0x111 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=0 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to '/content/OUTPUT.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf58.29.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj444p(pc, progressive), 612x357 [SAR 300:300 DAR 12:7], q=-1--1, 25 fps, 12800 tbn, 25 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc58.54.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "    Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 16000 Hz, mono, fltp, 69 kb/s\n",
            "    Metadata:\n",
            "      encoder         : Lavc58.54.100 aac\n",
            "frame=    1 fps=0.0 q=28.0 Lsize=     130kB time=00:00:14.01 bitrate=  75.8kbits/s speed= 170x    \n",
            "video:9kB audio:119kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 1.734392%\n",
            "\u001b[1;36m[libx264 @ 0x5577dcad8000] \u001b[0mframe I:1     Avg QP:25.94  size:  8221\n",
            "\u001b[1;36m[libx264 @ 0x5577dcad8000] \u001b[0mmb I  I16..4: 80.9%  0.0% 19.1%\n",
            "\u001b[1;36m[libx264 @ 0x5577dcad8000] \u001b[0mcoded y,u,v intra: 40.7% 13.2% 9.7%\n",
            "\u001b[1;36m[libx264 @ 0x5577dcad8000] \u001b[0mi16 v,h,dc,p:  7% 85%  6%  2%\n",
            "\u001b[1;36m[libx264 @ 0x5577dcad8000] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu:  7% 58% 11%  5%  2%  2%  3%  6%  5%\n",
            "\u001b[1;36m[libx264 @ 0x5577dcad8000] \u001b[0mkb/s:1644.20\n",
            "\u001b[1;36m[aac @ 0x5577dcad8d80] \u001b[0mQavg: 137.751\n"
          ]
        }
      ]
    }
  ]
}