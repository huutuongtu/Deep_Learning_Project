{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vfbyhs-xp9M_",
        "outputId": "b943a349-f3e5-45a8-8112-bd6d1879b8e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/PL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPvmE2N2qVp7",
        "outputId": "6e8a6dfa-a93b-4d81-efee-9f1aa1f1bc41"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/PL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install cmath\n",
        "!pip install torch\n",
        "!pip install sklearn\n",
        "!pip install pandas\n",
        "!pip install librosa\n",
        "!pip install numpy\n",
        "!pip install speechpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu8J17eyqNCg",
        "outputId": "4d2abe8b-adee-4db7-dbd7-a1f9fa1d91bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=436a5b8b8041dd9dc35db74d1d9c85d315dc8f3be8c33b45d4cc335498d4af5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/25/f7/1cc0956978ae479e75140219088deb7a36f60459df242b1a72\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.8/dist-packages (0.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (21.3)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.8/dist-packages (from librosa) (0.11.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.7.3)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.2.0)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (0.56.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.21.6)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (3.0.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from librosa) (0.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->librosa) (0.39.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->librosa) (6.0.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->librosa) (3.0.9)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.0->librosa) (2.25.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.8/dist-packages (from soundfile>=0.10.2->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (4.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.43.0->librosa) (3.11.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting speechpy\n",
            "  Downloading speechpy-2.4-py2.py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from speechpy) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from speechpy) (1.7.3)\n",
            "Installing collected packages: speechpy\n",
            "Successfully installed speechpy-2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install jiwer\n",
        "!pip install pyctcdecode\n",
        "from pyctcdecode import build_ctcdecoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43p2v4kztIcJ",
        "outputId": "ffc5355a-b5ff-4a31-da63-1aaca42b5e65"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-2.5.1-py3-none-any.whl (15 kB)\n",
            "Collecting levenshtein==0.20.2\n",
            "  Downloading Levenshtein-0.20.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
            "  Downloading rapidfuzz-2.13.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, levenshtein, jiwer\n",
            "Successfully installed jiwer-2.5.1 levenshtein-0.20.2 rapidfuzz-2.13.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyctcdecode\n",
            "  Downloading pyctcdecode-0.5.0-py2.py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from pyctcdecode) (1.21.6)\n",
            "Collecting pygtrie<3.0,>=2.1\n",
            "  Downloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
            "Collecting hypothesis<7,>=6.14\n",
            "  Downloading hypothesis-6.64.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.1/401.1 KB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from hypothesis<7,>=6.14->pyctcdecode) (2.4.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/dist-packages (from hypothesis<7,>=6.14->pyctcdecode) (22.2.0)\n",
            "Collecting exceptiongroup>=1.0.0\n",
            "  Downloading exceptiongroup-1.1.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pygtrie, exceptiongroup, hypothesis, pyctcdecode\n",
            "Successfully installed exceptiongroup-1.1.0 hypothesis-6.64.0 pyctcdecode-0.5.0 pygtrie-2.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyctcdecode.language_model:kenlm python bindings are not installed. Most likely you want to install it using: pip install https://github.com/kpu/kenlm/archive/master.zip\n",
            "WARNING:pyctcdecode.decoder:kenlm python bindings are not installed. Most likely you want to install it using: pip install https://github.com/kpu/kenlm/archive/master.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# import pandas as pd\n",
        "# data = pd.read_csv(\"/content/drive/MyDrive/PL/metadata_train.csv\")\n",
        "\n",
        "# train,test = train_test_split(data, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "ZkmstX7ujyi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train.to_csv(\"/content/drive/MyDrive/PL/train.csv\")\n",
        "# test.to_csv(\"/content/drive/MyDrive/PL/dev.csv\")"
      ],
      "metadata": {
        "id": "z3dP8RlBkHuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from cmath import acos\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "# import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "# import numpy as np\n",
        "# from torch.utils.data import DataLoader\n",
        "# from infer import phonetic_embedding\n",
        "from help import Atention, wav_norm\n",
        "from char_embedding import tensor_to_text,text_to_tensor\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv(\"train.csv\")\n",
        "# print(data['Wav_path'])\n",
        "sample = data.shape[0]\n",
        "# cols = ['Wav_path', 'Lyric_path']\n",
        "\n",
        "class MDD_Dataset(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "        acoustic_canonical = data\n",
        "        self.n_samples = sample\n",
        "        A = acoustic_canonical['Wav_path']\n",
        "        C = acoustic_canonical['Lyric_path']\n",
        "        B = acoustic_canonical['Lyric_path'] #output\n",
        "        \n",
        "\n",
        "        self.A_data = A \n",
        "        self.C_data = C\n",
        "        self.y_data = B \n",
        "\n",
        "    # support indexing such that dataset[i] can be used to get i-th sample\n",
        "    def __getitem__(self, index):\n",
        "        p = self.A_data[index]\n",
        "        p = p.split(\".\")[0]\n",
        "        base_dir = '/content/drive/MyDrive/vocal_song_phonetic/'\n",
        "        p = p + str(\".npy\")\n",
        "        p = np.load(base_dir + str(p))\n",
        "        phonetic = torch.tensor(p)\n",
        "       \n",
        "        linguistic = text_to_tensor(self.C_data[index])\n",
        "        linguistic = torch.tensor(linguistic)\n",
        "        label = text_to_tensor(self.y_data[index])\n",
        "        label = torch.tensor(label)\n",
        "        return phonetic, linguistic, label\n",
        "\n",
        "    # we can call len(dataset) to return the size\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n"
      ],
      "metadata": {
        "id": "sfQTy9LZqs6i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from model import Net\n",
        "# from cProfile import label\n",
        "# from infer import phonetic_embedding\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from phonetic_encoder import Phonetic_encoder\n",
        "from acoustic_encoder import Acoustic_encoder\n",
        "from linguistic_encoder import Linguistic_encoder\n",
        "from char_embedding import text_to_tensor, clean_corpus\n",
        "from help import wav_norm, Atention\n",
        "import numpy as np\n",
        "# from help import beam_search_decoding\n",
        "from model import Acoustic_Phonetic_Linguistic\n",
        "# from dataloader import MDD_Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from jiwer import wer, cer\n",
        "tokenizer = Wav2Vec2Processor.from_pretrained(\"pretrained_finetuned\")\n",
        "\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "f = open(\"/content/drive/MyDrive/PL/vocal_loss.txt\", 'a')\n",
        "MDD = MDD_Dataset()\n",
        "# print(data)\n",
        "net = Acoustic_Phonetic_Linguistic()\n",
        "net.to('cuda')\n",
        "\n",
        "dev = pd.read_csv(\"dev.csv\")\n",
        "# net = torch.load('/content/drive/MyDrive/PL/raw_checkpoint.pth14')\n",
        "# net = net.to('cpu')\n",
        "train_loader = DataLoader(dataset=MDD,\n",
        "                          batch_size=1,\n",
        "                          shuffle=True,\n",
        "                          num_workers=0)\n",
        "\n",
        "\n",
        "\n",
        "# convert to an iterator and look at one random sample\n",
        "\n",
        "\n",
        "ctc_loss = nn.CTCLoss(blank = 95)\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.00001, momentum=0.9)\n",
        "optimizer = optim.AdamW(net.parameters(), lr = 0.00001)\n",
        "# optimizer = optim.SGD(net.parameters(), 0.01, momentum = 0.9)\n",
        "# optimizer = torch.load('/home/tuht/train_wav2vec/MDD_Checkpoint/checkpoint_optim.pth')\n",
        "best_WER = 100\n",
        "for epoch in range(50):  # loop over the dataset multiple times\n",
        "    net.train().to('cuda')\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader):\n",
        "        phonetic, linguistic, labels = data\n",
        "        # acoustic = torch.tensor(0).to('cuda')\n",
        "        phonetic = phonetic.to('cuda')\n",
        "        linguistic = linguistic.to('cuda')\n",
        "        labels = labels.to('cuda')\n",
        "        optimizer.zero_grad()\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(phonetic, linguistic)\n",
        "        outputs = outputs.unsqueeze(1)\n",
        "        input_lengths = outputs.shape\n",
        "        target_lengths = labels.shape\n",
        "        target = labels\n",
        "        input_lengths = [input_lengths[0]]\n",
        "        target_lengths =[target_lengths[1]]\n",
        "        input_lengths = torch.tensor(input_lengths)\n",
        "        target_lengths = torch.tensor(target_lengths)\n",
        "        outputs = (F.log_softmax(outputs, dim=2))\n",
        "        loss = ctc_loss(outputs, labels, input_lengths, target_lengths)\n",
        "        # print(i)\n",
        "        # print(loss)\n",
        "        f.write(\"(\" +str(epoch) + \",\" + str(i) + \")\" + \"  loss: \" + str(loss) + \"\\n\") \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    WER = 0\n",
        "    net.eval().to('cuda')\n",
        "    for i in range(len(dev)):\n",
        "        p = dev['Wav_path'][i]\n",
        "        p = p.split(\".\")[0]\n",
        "        base_dir = '/content/drive/MyDrive/vocal_song_phonetic/'\n",
        "        p = p + str(\".npy\")\n",
        "        p = np.load(base_dir + str(p))\n",
        "        phonetic = torch.tensor(p)\n",
        "       \n",
        "        linguistic = text_to_tensor(dev['Lyric_path'][i])\n",
        "        linguistic = torch.tensor(linguistic)\n",
        "        label = text_to_tensor(dev['Lyric_path'][i])\n",
        "        label = torch.tensor(label)\n",
        "        phonetic = phonetic.to('cuda')\n",
        "        linguistic = linguistic.to('cuda')\n",
        "        phonetic = phonetic.unsqueeze(0)\n",
        "        linguistic = linguistic.unsqueeze(0)\n",
        "        outputs = net(phonetic, linguistic)\n",
        "        outputs = outputs.unsqueeze(0)\n",
        "        x = F.log_softmax(outputs,dim=2)\n",
        "        labels = ['ắ', 'ồ', 'z', 'ứ', 'ỡ', 'ì', 'x', 'ặ', 'u', 'ẹ', 'd', 'ỵ', 'r', 'p', 't', 'ỳ', 'ẩ', 'f', 'ó', 'á', 'v', 'ã', 'i', 'ư', 'ở', 'ễ', 'ụ', 'ú', 'ũ', ' ', 'ă', 'é', 'ằ', 'a', 'ấ', 'ờ', 'ữ', 'ớ', 'n', 'ý', 's', 'h', 'ơ', 'ị', 'l', 'c', 'k', 'ỷ', 'ỗ', 'ế', 'ẻ', 'ợ', 'ẫ', 'í', 'ỏ', 'ủ', 'g', 'q', 'j', 'ò', 'ỹ', 'ự', 'ô', 'b', 'y', 'ĩ', 'ỉ', 'ẵ', 'ầ', 'ê', 'ộ', 'ậ', 'm', 'ń', 'o', 'ọ', 'đ', 'ẽ', 'ử', 'à', 'è', 'e', 'ẳ', 'ổ', 'ù', 'w', 'ả', 'ạ', 'â', 'ệ', 'ề', 'õ', 'ố', 'ể', 'ừ',]\n",
        "        x = x.detach().cpu().numpy()\n",
        "        decoder = build_ctcdecoder(\n",
        "            labels = labels,\n",
        "            \n",
        "        )\n",
        "        x = x.squeeze(0)\n",
        "        ground_truth = (clean_corpus(dev['Lyric_path'][i]))\n",
        "        hypothesis = str(decoder.decode(x))\n",
        "        error = cer(ground_truth, hypothesis)\n",
        "        WER = WER + error\n",
        "    print(epoch)\n",
        "    print(WER)\n",
        "    if WER/len(dev)<best_WER:\n",
        "      print(\"save \" + str(epoch))\n",
        "      # print(epoch)\n",
        "      # print(WER)\n",
        "      best_WER = WER/len(dev)\n",
        "      print(best_WER)\n",
        "      torch.save(net, '/content/drive/MyDrive/PL/MDD_Checkpoint/vocal_checkpoint.pth')\n",
        "        \n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "barzAew9tAev",
        "outputId": "cadad485-d0a7-4268-c9da-6ac0408dd4e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/configuration_utils.py:369: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/content/drive/MyDrive/PL/linguistic_encoder.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "51.90985808747225\n",
            "save 0\n",
            "0.4897156423346439\n",
            "1\n",
            "46.68650132502654\n",
            "save 1\n",
            "0.4404386917455334\n",
            "2\n",
            "45.365100795686025\n",
            "save 2\n",
            "0.4279726490159059\n",
            "3\n",
            "45.05404392687716\n",
            "save 3\n",
            "0.4250381502535581\n",
            "4\n",
            "44.694133905946735\n",
            "save 4\n",
            "0.4216427726976107\n",
            "5\n",
            "44.57325048917713\n",
            "save 5\n",
            "0.4205023631054446\n",
            "6\n",
            "44.29809137715863\n",
            "save 6\n",
            "0.4179065224260248\n",
            "7\n",
            "44.28136764063459\n",
            "save 7\n",
            "0.41774875132674144\n",
            "8\n",
            "44.18679456902086\n",
            "save 8\n",
            "0.41685655253793263\n",
            "9\n",
            "43.96395431707662\n",
            "save 9\n",
            "0.4147542860101568\n",
            "10\n",
            "43.76067376546919\n",
            "save 10\n",
            "0.4128365449572565\n",
            "11\n",
            "44.000565851975004\n",
            "12\n",
            "44.179314302305315\n",
            "13\n",
            "43.59082420376029\n",
            "save 13\n",
            "0.4112341906015122\n",
            "14\n",
            "43.599868162697874\n",
            "15\n",
            "43.330021766920616\n",
            "save 15\n",
            "0.4087737902539681\n",
            "16\n",
            "43.48002122245216\n",
            "17\n",
            "43.712386581049316\n",
            "18\n",
            "43.54815115994965\n",
            "19\n",
            "43.43260320076995\n",
            "20\n",
            "43.74174828257709\n",
            "21\n",
            "43.35992363368725\n",
            "22\n",
            "43.280467586419526\n",
            "save 22\n",
            "0.4083062979850899\n",
            "23\n",
            "43.26112317357824\n",
            "save 23\n",
            "0.40812380352432304\n",
            "24\n",
            "43.759810726054106\n",
            "25\n",
            "43.1824265399046\n",
            "save 25\n",
            "0.4073813824519302\n",
            "26\n",
            "43.808792786901556\n",
            "27\n",
            "43.35036258244608\n",
            "28\n",
            "43.55425549171284\n",
            "29\n",
            "43.49276242205124\n",
            "30\n",
            "43.29401460902201\n",
            "31\n",
            "43.14129662360092\n",
            "save 31\n",
            "0.4069933643735936\n",
            "32\n",
            "43.08291172134717\n",
            "save 32\n",
            "0.4064425634089356\n",
            "33\n",
            "43.062444822916966\n",
            "save 33\n",
            "0.4062494794614808\n",
            "34\n",
            "43.08635585070228\n",
            "35\n",
            "43.41735274415119\n",
            "36\n",
            "43.53565294347149\n",
            "37\n",
            "42.891188355432064\n",
            "save 37\n",
            "0.4046338524097364\n",
            "38\n",
            "43.15548780126493\n",
            "39\n",
            "43.16040303703973\n",
            "40\n",
            "42.66050585763798\n",
            "save 40\n",
            "0.40245760243054696\n",
            "41\n",
            "43.23010980211507\n",
            "42\n",
            "42.93495351075062\n",
            "43\n",
            "43.4637772261502\n",
            "44\n",
            "42.91679815283506\n",
            "45\n",
            "42.83753669663203\n",
            "46\n",
            "42.55049217878393\n",
            "save 46\n",
            "0.40141973753569743\n",
            "47\n",
            "42.698366612553585\n",
            "48\n",
            "42.855667485673095\n",
            "49\n",
            "43.17565211862319\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from infer import phonetic_embedding\n",
        "\n",
        "net = torch.load('/content/drive/MyDrive/PL/MDD_Checkpoint/vocal_checkpoint.pth')\n",
        "net.eval().to('cuda')\n",
        "path = '/content/37303234365f3231.wav'\n",
        "lyrics = 'Trăngcôđơnlặnglẽ Nhớaiquatừng Gócphốngàyxưa Nghetrongtimvụnvỡ Baolầnnáttanvìai Cóainhớngày Xưaemơi Giờđãxatôimấtrồi Ánhtrăngsáng Lònganhemơi Giờthấmướt'\n",
        "phonetic = phonetic_embedding(path).squeeze(0)\n",
        "linguistic = text_to_tensor(lyrics)\n",
        "linguistic = torch.tensor(linguistic)\n",
        "label = text_to_tensor(dev['Lyric_path'][i])\n",
        "label = torch.tensor(label)\n",
        "phonetic = phonetic.to('cuda')\n",
        "linguistic = linguistic.to('cuda')\n",
        "phonetic = phonetic.unsqueeze(0)\n",
        "linguistic = linguistic.unsqueeze(0)\n",
        "outputs = net(phonetic, linguistic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2R9vgdedvmAS",
        "outputId": "00e0e8bb-f294-4cd8-a3d6-cf6cad97b2af"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/PL/linguistic_encoder.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = outputs.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "AwCgThH-AaDL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"/content/output.npy\", outputs)"
      ],
      "metadata": {
        "id": "PSyqpayCAQ68"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# /content/drive/MyDrive/output_alignment_fusion/3130303538355f3338.npy\n",
        "!pip install torch\n",
        "# from torch.utils.data import Dataset\n",
        "# import matplotlib.pyplot as plt\n",
        "# import os\n",
        "# import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "# import numpy as np\n",
        "# from char_embedding import tensor_to_text\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "\n",
        "\n",
        "x = np.load(\"/content/output.npy\")\n",
        "\n",
        "# print(x)\n",
        "\n",
        "x = torch.tensor(x)\n",
        "predicted_ids = torch.argmax(torch.tensor(x), dim=1)\n",
        "\n",
        "\n",
        "x = torch.log_softmax(x, dim=-1)\n",
        "x = x.cpu().detach()\n",
        "\n",
        "\n",
        "labels = ('ắ', 'ồ', 'z', 'ứ', 'ỡ', 'ì', 'x', 'ặ', 'u', 'ẹ', 'd', 'ỵ', 'r', 'p', 't', 'ỳ', 'ẩ', 'f', 'ó', 'á', 'v', 'ã', 'i', 'ư', 'ở', 'ễ', 'ụ', 'ú', 'ũ', ' ', 'ă', 'é', 'ằ', 'a', 'ấ', 'ờ', 'ữ', 'ớ', 'n', 'ý', 's', 'h', 'ơ', 'ị', 'l', 'c', 'k', 'ỷ', 'ỗ', 'ế', 'ẻ', 'ợ', 'ẫ', 'í', 'ỏ', 'ủ', 'g', 'q', 'j', 'ò', 'ỹ', 'ự', 'ô', 'b', 'y', 'ĩ', 'ỉ', 'ẵ', 'ầ', 'ê', 'ộ', 'ậ', 'm', 'ń', 'o', 'ọ', 'đ', 'ẽ', 'ử', 'à', 'è', 'e', 'ẳ', 'ổ', 'ù', 'w', 'ả', 'ạ', 'â', 'ệ', 'ề', 'õ', 'ố', 'ể', 'ừ',)\n",
        "\n",
        "def clean_corpus(str1):\n",
        "    res1 = \"\"\n",
        "    for i in str1:\n",
        "        if i.isalpha() or i==\" \":\n",
        "            res1 = \"\".join([res1, i])\n",
        "    return res1.lower()\n",
        "transcript = \" \" + 'Trăng cô đơn lặng lẽ Nhớ ai qua từng Góc phố ngày xưa Nghe trong tim vụn vỡ Bao lần nát tan vì ai Có ai nhớ ngày Xưa em ơi Giờ đã xa tôi mất rồi Ánh trăng sáng Lòng anh em ơi Giờ thấm ướt' + \"\"\n",
        "transcript = clean_corpus(transcript)\n",
        "dictionary = {c: i for i, c in enumerate(labels)}\n",
        "\n",
        "tokens = [dictionary[c] for c in transcript]\n",
        "\n",
        "\n",
        "def get_trellis(x, tokens, blank_id=95):\n",
        "    num_frame = x.size(0)\n",
        "    num_tokens = len(tokens)\n",
        "\n",
        "    # Trellis has extra diemsions for both time axis and tokens.\n",
        "    # The extra dim for tokens represents <SoS> (start-of-sentence)\n",
        "    # The extra dim for time axis is for simplification of the code.\n",
        "    trellis = torch.empty((num_frame + 1, num_tokens + 1))\n",
        "    trellis[0, 0] = 0\n",
        "    trellis[1:, 0] = torch.cumsum(x[:, 0], 0)\n",
        "    trellis[0, -num_tokens:] = -float(\"inf\")\n",
        "    trellis[-num_tokens:, 0] = float(\"inf\")\n",
        "\n",
        "    for t in range(num_frame):\n",
        "        trellis[t + 1, 1:] = torch.maximum(\n",
        "            # Score for staying at the same token\n",
        "            trellis[t, 1:] + x[t, blank_id],\n",
        "            # Score for changing to the next token\n",
        "            trellis[t, :-1] + x[t, tokens],\n",
        "        )\n",
        "    return trellis\n",
        "\n",
        "\n",
        "trellis = get_trellis(x, tokens)\n",
        "\n",
        "from dataclasses import dataclass\n",
        "@dataclass\n",
        "class Point:\n",
        "  token_index: int\n",
        "  time_index: int\n",
        "  score: float\n",
        "\n",
        "\n",
        "def backtrack(trellis, emission, tokens, blank_id=95):\n",
        "  # Note:\n",
        "  # j and t are indices for trellis, which has extra dimensions \n",
        "  # for time and tokens at the beginning.\n",
        "  # When refering to time frame index `T` in trellis,\n",
        "  # the corresponding index in emission is `T-1`.\n",
        "  # Similarly, when refering to token index `J` in trellis,\n",
        "  # the corresponding index in transcript is `J-1`.\n",
        "  j = trellis.size(1) - 1\n",
        "  t_start = torch.argmax(trellis[:, j]).item()\n",
        "\n",
        "  path = []\n",
        "  for t in range(t_start, 0, -1):\n",
        "    # 1. Figure out if the current position was stay or change\n",
        "    # Note (again):\n",
        "    # `emission[J-1]` is the emission at time frame `J` of trellis dimension.\n",
        "    # Score for token staying the same from time frame J-1 to T.\n",
        "    stayed = trellis[t-1, j] + emission[t-1, blank_id]\n",
        "    # Score for token changing from C-1 at T-1 to J at T.\n",
        "    changed = trellis[t-1, j-1] + emission[t-1, tokens[j-1]]\n",
        "\n",
        "    # 2. Store the path with frame-wise probability.\n",
        "    prob = emission[t-1, tokens[j-1] if changed > stayed else 0].exp().item()\n",
        "    # Return token index and time index in non-trellis coordinate.\n",
        "    path.append(Point(j-1, t-1, prob))\n",
        "\n",
        "    # 3. Update the token\n",
        "    if changed > stayed:\n",
        "      j -= 1\n",
        "      if j == 0:\n",
        "        break\n",
        "  else:\n",
        "    raise ValueError('Failed to align')\n",
        "  return path[::-1]\n",
        "\n",
        "path = backtrack(trellis, x, tokens)\n",
        "@dataclass\n",
        "class Segment:\n",
        "    label: str\n",
        "    start: int\n",
        "    end: int\n",
        "    score: float\n",
        "    \n",
        "\n",
        "    def __repr__(self):\n",
        "      return f\"{self.start} --> {self.end} {self.label} \\n\"\n",
        "\n",
        "    @property\n",
        "    def length(self):\n",
        "        return self.end - self.start\n",
        "\n",
        "\n",
        "def merge_repeats(path):\n",
        "    i1, i2 = 0, 0\n",
        "    segments = []\n",
        "    while i1 < len(path):\n",
        "        while i2 < len(path) and path[i1].token_index == path[i2].token_index:\n",
        "            i2 += 1\n",
        "        score = sum(path[k].score for k in range(i1, i2)) / (i2 - i1)\n",
        "        segments.append(\n",
        "            Segment(\n",
        "                transcript[path[i1].token_index],\n",
        "                path[i1].time_index,\n",
        "                path[i2 - 1].time_index + 1,\n",
        "                score,\n",
        "            )\n",
        "        )\n",
        "        i1 = i2\n",
        "    return segments\n",
        "\n",
        "\n",
        "segments = merge_repeats(path)\n",
        "\n",
        "# Merge words\n",
        "def merge_words(segments, separator=\" \"):\n",
        "    words = []\n",
        "    i1, i2 = 0, 0\n",
        "    while i1 < len(segments):\n",
        "        if i2 >= len(segments) or segments[i2].label == separator:\n",
        "            if i1 != i2:\n",
        "                segs = segments[i1:i2]\n",
        "                word = \"\".join([seg.label for seg in segs])\n",
        "                score = sum(seg.score * seg.length for seg in segs) / sum(seg.length for seg in segs)\n",
        "                words.append(Segment(word, segments[i1].start*20, segments[i2 - 1].end*20, score))\n",
        "            i1 = i2 + 1\n",
        "            i2 = i1\n",
        "        else:\n",
        "            i2 += 1\n",
        "    return words\n",
        "\n",
        "\n",
        "word_segments = merge_words(segments)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykBfv3fBAnU5",
        "outputId": "a12cd248-1d76-4c19-8e77-9b6bdd681bfd"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-e630c79d24bf>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  predicted_ids = torch.argmax(torch.tensor(x), dim=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_segments[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ONaFso-A7Sd",
        "outputId": "85e1b313-a035-412a-d847-9a292ba2a1a9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.180000 --> 0.520000 trăng \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(str(word_segments[0]).split(\" \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvLb-mptCr2W",
        "outputId": "fdddd747-e785-4283-eda2-f88e06b769ee"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['180.000000', '-->', '520.000000', 'trăng', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_millis(milliseconds):\n",
        "  millis = milliseconds\n",
        "  millisecond = int(millis%1000)\n",
        "\n",
        "  seconds=(millis/1000)%60\n",
        "  seconds = int(seconds)\n",
        "  minutes=(millis/(1000*60))%60\n",
        "  minutes = int(minutes)\n",
        "  hours=(millis/(1000*60*60))%24\n",
        "\n",
        "  return (\"%d:%d:%d,%d\" % (hours, minutes, seconds, millisecond))"
      ],
      "metadata": {
        "id": "u-hbfwocC_BH"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"/content/output.srt\", \"a\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rogket9PES8W",
        "outputId": "2c3f30b6-2bd4-4ddd-f0ea-bd0d54a947a4"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:0:2,500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(word_segments)):\n",
        "  word_segments[i] = str(word_segments[i])\n",
        "  start = word_segments[i].split(\" \")[0]\n",
        "  end = word_segments[i].split(\" \")[2]\n",
        "  word = word_segments[i].split(\" \")[3]\n",
        "\n",
        "  f.write(str(i+1) + \"\\n\" + str(format_millis(int(start))) + \" --> \" + str(format_millis(int(end))) + \"\\n\" + str(word) + \"\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp5tEDqhCTd_",
        "outputId": "838120ec-e5ed-4588-e8fa-f11289a2f586"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:0:0,180\n",
            "0:0:0,520\n",
            "trăng\n",
            "0:0:0,540\n",
            "0:0:0,700\n",
            "cô\n",
            "0:0:0,840\n",
            "0:0:1,40\n",
            "đơn\n",
            "0:0:1,160\n",
            "0:0:1,340\n",
            "lặng\n",
            "0:0:1,460\n",
            "0:0:1,640\n",
            "lẽ\n",
            "0:0:1,800\n",
            "0:0:2,80\n",
            "nhớ\n",
            "0:0:2,200\n",
            "0:0:2,280\n",
            "ai\n",
            "0:0:2,400\n",
            "0:0:2,580\n",
            "qua\n",
            "0:0:2,720\n",
            "0:0:2,980\n",
            "từng\n",
            "0:0:3,40\n",
            "0:0:3,180\n",
            "góc\n",
            "0:0:3,340\n",
            "0:0:3,620\n",
            "phố\n",
            "0:0:3,680\n",
            "0:0:3,880\n",
            "ngày\n",
            "0:0:4,120\n",
            "0:0:5,60\n",
            "xưa\n",
            "0:0:5,220\n",
            "0:0:5,420\n",
            "nghe\n",
            "0:0:5,540\n",
            "0:0:5,720\n",
            "trong\n",
            "0:0:5,880\n",
            "0:0:6,140\n",
            "tim\n",
            "0:0:6,200\n",
            "0:0:6,440\n",
            "vụn\n",
            "0:0:6,500\n",
            "0:0:6,900\n",
            "vỡ\n",
            "0:0:6,980\n",
            "0:0:7,340\n",
            "bao\n",
            "0:0:7,440\n",
            "0:0:7,680\n",
            "lần\n",
            "0:0:7,740\n",
            "0:0:7,960\n",
            "nát\n",
            "0:0:8,80\n",
            "0:0:8,620\n",
            "tan\n",
            "0:0:8,720\n",
            "0:0:9,0\n",
            "vì\n",
            "0:0:9,180\n",
            "0:0:10,220\n",
            "ai\n",
            "0:0:10,300\n",
            "0:0:10,480\n",
            "có\n",
            "0:0:10,740\n",
            "0:0:10,840\n",
            "ai\n",
            "0:0:10,960\n",
            "0:0:11,140\n",
            "nhớ\n",
            "0:0:11,220\n",
            "0:0:11,380\n",
            "ngày\n",
            "0:0:11,520\n",
            "0:0:11,720\n",
            "xưa\n",
            "0:0:11,840\n",
            "0:0:11,960\n",
            "em\n",
            "0:0:12,160\n",
            "0:0:12,580\n",
            "ơi\n",
            "0:0:12,660\n",
            "0:0:12,820\n",
            "giờ\n",
            "0:0:12,840\n",
            "0:0:13,20\n",
            "đã\n",
            "0:0:13,120\n",
            "0:0:13,280\n",
            "xa\n",
            "0:0:13,360\n",
            "0:0:13,580\n",
            "tôi\n",
            "0:0:13,640\n",
            "0:0:13,800\n",
            "mất\n",
            "0:0:14,0\n",
            "0:0:15,220\n",
            "rồi\n",
            "0:0:15,420\n",
            "0:0:15,500\n",
            "ánh\n",
            "0:0:15,580\n",
            "0:0:15,720\n",
            "trăng\n",
            "0:0:15,860\n",
            "0:0:16,80\n",
            "sáng\n",
            "0:0:16,260\n",
            "0:0:16,520\n",
            "lòng\n",
            "0:0:16,660\n",
            "0:0:16,720\n",
            "anh\n",
            "0:0:16,920\n",
            "0:0:17,20\n",
            "em\n",
            "0:0:17,220\n",
            "0:0:17,280\n",
            "ơi\n",
            "0:0:17,520\n",
            "0:0:17,680\n",
            "giờ\n",
            "0:0:17,840\n",
            "0:0:18,20\n",
            "thấm\n",
            "0:0:18,140\n",
            "0:0:18,200\n",
            "ướt\n"
          ]
        }
      ]
    }
  ]
}